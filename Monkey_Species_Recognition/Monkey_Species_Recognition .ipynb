{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#image Directory/path\n",
    "\n",
    "base_dir ='Monkey'\n",
    "train_dir = os.path.join(base_dir, 'training')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# Directory with our training Monkey_Species (n0~n9) pictures\n",
    "train_n0_dir = os.path.join(train_dir, 'n0')\n",
    "train_n1_dir = os.path.join(train_dir, 'n1')\n",
    "train_n2_dir = os.path.join(train_dir, 'n2')\n",
    "train_n3_dir = os.path.join(train_dir, 'n3')\n",
    "train_n4_dir = os.path.join(train_dir, 'n4')\n",
    "train_n5_dir = os.path.join(train_dir, 'n5')\n",
    "train_n6_dir = os.path.join(train_dir, 'n6')\n",
    "train_n7_dir = os.path.join(train_dir, 'n7')\n",
    "train_n8_dir = os.path.join(train_dir, 'n8')\n",
    "train_n9_dir = os.path.join(train_dir, 'n9')\n",
    "\n",
    "\n",
    "# Directory with our validation Monkey_Species (n0~n9) pictures\n",
    "validation_n0_dir = os.path.join(validation_dir, 'n0')\n",
    "validation_n1_dir = os.path.join(validation_dir, 'n1')\n",
    "validation_n2_dir = os.path.join(validation_dir, 'n2')\n",
    "validation_n3_dir = os.path.join(validation_dir, 'n3')\n",
    "validation_n4_dir = os.path.join(validation_dir, 'n4')\n",
    "validation_n5_dir = os.path.join(validation_dir, 'n5')\n",
    "validation_n6_dir = os.path.join(validation_dir, 'n6')\n",
    "validation_n7_dir = os.path.join(validation_dir, 'n7')\n",
    "validation_n8_dir = os.path.join(validation_dir, 'n8')\n",
    "validation_n9_dir = os.path.join(validation_dir, 'n9')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n0018.jpg', 'n0019.jpg', 'n0020.jpg', 'n0021.jpg', 'n0022.jpg']\n",
      "['n1017.jpg', 'n1018.jpg', 'n1019.jpg', 'n1021.jpg', 'n1022.jpg']\n",
      "['n2017.jpg', 'n2018.jpg', 'n2019.jpg', 'n2020.jpg', 'n2021.jpg']\n",
      "['n3020.jpg', 'n3021.jpg', 'n3022.jpg', 'n3023.jpg', 'n3024.jpg']\n"
     ]
    }
   ],
   "source": [
    "train_n0_fnames = os.listdir( train_n0_dir )\n",
    "train_n1_fnames = os.listdir( train_n1_dir )\n",
    "train_n2_fnames = os.listdir( train_n2_dir )\n",
    "train_n3_fnames = os.listdir( train_n3_dir )\n",
    "train_n4_fnames = os.listdir( train_n4_dir )\n",
    "train_n5_fnames = os.listdir( train_n5_dir )\n",
    "train_n6_fnames = os.listdir( train_n6_dir )\n",
    "train_n7_fnames = os.listdir( train_n7_dir )\n",
    "train_n8_fnames = os.listdir( train_n8_dir )\n",
    "train_n9_fnames = os.listdir( train_n9_dir )\n",
    "\n",
    "# print four species image\n",
    "\n",
    "print(train_n0_fnames[:5])\n",
    "print(train_n1_fnames[:5])\n",
    "print(train_n2_fnames[:5])\n",
    "print(train_n3_fnames[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training n0 images : 105\n",
      "total training n1 images : 111\n",
      "total training n2 images : 110\n",
      "total training n3 images : 122\n",
      "total training n4 images : 105\n",
      "total training n5 images : 113\n",
      "total training n6 images : 106\n",
      "total training n7 images : 114\n",
      "total training n8 images : 106\n",
      "total training n9 images : 106\n",
      "total test n0 images : 26\n",
      "total test n1 images : 28\n",
      "total test n2 images : 27\n",
      "total test n3 images : 30\n",
      "total test n4 images : 26\n",
      "total test n5 images : 28\n",
      "total test n6 images : 26\n",
      "total test n7 images : 28\n",
      "total test n8 images : 27\n",
      "total test n9 images : 26\n"
     ]
    }
   ],
   "source": [
    "print('total training n0 images :', len(os.listdir(train_n0_dir)))\n",
    "print('total training n1 images :', len(os.listdir(train_n1_dir)))\n",
    "print('total training n2 images :', len(os.listdir(train_n2_dir)))\n",
    "print('total training n3 images :', len(os.listdir(train_n3_dir)))\n",
    "print('total training n4 images :', len(os.listdir(train_n4_dir)))\n",
    "print('total training n5 images :', len(os.listdir(train_n5_dir)))\n",
    "print('total training n6 images :', len(os.listdir(train_n6_dir)))\n",
    "print('total training n7 images :', len(os.listdir(train_n7_dir)))\n",
    "print('total training n8 images :', len(os.listdir(train_n8_dir)))\n",
    "print('total training n9 images :', len(os.listdir(train_n9_dir)))\n",
    "\n",
    "\n",
    "print('total test n0 images :', len(os.listdir(validation_n0_dir)))\n",
    "print('total test n1 images :', len(os.listdir(validation_n1_dir)))\n",
    "print('total test n2 images :', len(os.listdir(validation_n2_dir)))\n",
    "print('total test n3 images :', len(os.listdir(validation_n3_dir)))\n",
    "print('total test n4 images :', len(os.listdir(validation_n4_dir)))\n",
    "print('total test n5 images :', len(os.listdir(validation_n5_dir)))\n",
    "print('total test n6 images :', len(os.listdir(validation_n6_dir)))\n",
    "print('total test n7 images :', len(os.listdir(validation_n7_dir)))\n",
    "print('total test n8 images :', len(os.listdir(validation_n8_dir)))\n",
    "print('total test n9 images :', len(os.listdir(validation_n9_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "pic_index = 0 # Index for iterating over images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x1152 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols*4, nrows*4)\n",
    "\n",
    "pic_index+=5\n",
    "\n",
    "# print four monkey species image\n",
    "\n",
    "next_n0_pix = [os.path.join(train_n0_dir, fname) \n",
    "                for fname in train_n0_fnames[ pic_index-8:pic_index] \n",
    "               ]\n",
    "\n",
    "next_n1_pix = [os.path.join(train_n1_dir, fname) \n",
    "                for fname in train_n1_fnames[ pic_index-8:pic_index]\n",
    "               ]\n",
    "next_n1_pix = [os.path.join(train_n2_dir, fname) \n",
    "                for fname in train_n2_fnames[ pic_index-8:pic_index]\n",
    "               ]\n",
    "next_n1_pix = [os.path.join(train_n3_dir, fname) \n",
    "                for fname in train_n3_fnames[ pic_index-8:pic_index]\n",
    "               ]\n",
    "\n",
    "for i, img_path in enumerate(next_n0_pix+next_n1_pix):\n",
    "  # Set up subplot; subplot indices start at 1\n",
    "  sp = plt.subplot(nrows, ncols, i + 1)\n",
    "  sp.axis('Off') # Don't show axes (or gridlines)\n",
    "\n",
    "  img = mpimg.imread(img_path)\n",
    "  plt.imshow(img)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tasli\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\tasli\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\tasli\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\tasli\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\tasli\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\tasli\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\tasli\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\tasli\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\tasli\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\tasli\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\tasli\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\tasli\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1)\n",
      "(1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "training_images = np.expand_dims(train_dir, axis=(0,1,2)) # Your Code Here\n",
    "validation_images = np.expand_dims(validation_dir, axis=(0,1,2)) # Your Code Here\n",
    "\n",
    "# Create an ImageDataGenerator and do Image Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    "   \n",
    "    )\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255)\n",
    " \n",
    "\n",
    "print(training_images.shape)\n",
    "print(validation_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Four layer D-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tasli\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # the input shape of image 150x150 (convert 400x300 by 150x150 ) with 3 bytes color\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2), \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'), \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(), \n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'), \n",
    "    tf.keras.layers.Dense(10, activation='softmax')  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 3,314,346\n",
      "Trainable params: 3,314,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1098 images belonging to 10 classes.\n",
      "Found 272 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255.\n",
    "train_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
    "test_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
    "\n",
    "\n",
    "# Flow training images in batches of 100 using train_datagen generator\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    \n",
    "                                                    class_mode='categorical',\n",
    "                                                    target_size=(150, 150))     \n",
    "\n",
    "# Flow validation images in batches of 100 using test_datagen generator\n",
    "\n",
    "validation_generator =  test_datagen.flow_from_directory(validation_dir,\n",
    "                                                        \n",
    "                                                         class_mode  = 'categorical',\n",
    "                                                         target_size = (150, 150))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 625s 18s/step - loss: 1.9226 - acc: 0.3005 - val_loss: 1.8246 - val_acc: 0.3750\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 472s 13s/step - loss: 1.5783 - acc: 0.4235 - val_loss: 1.7365 - val_acc: 0.3640\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 535s 15s/step - loss: 1.3267 - acc: 0.5282 - val_loss: 1.4873 - val_acc: 0.4743\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 522s 15s/step - loss: 1.0500 - acc: 0.6412 - val_loss: 1.3451 - val_acc: 0.5625\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 530s 15s/step - loss: 0.8258 - acc: 0.7332 - val_loss: 1.1931 - val_acc: 0.5735\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 732s 21s/step - loss: 0.5295 - acc: 0.8233 - val_loss: 1.2821 - val_acc: 0.6213\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 572s 16s/step - loss: 0.3815 - acc: 0.8798 - val_loss: 1.7354 - val_acc: 0.5735\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 537s 15s/step - loss: 0.3680 - acc: 0.8752 - val_loss: 1.3737 - val_acc: 0.6324\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 746s 21s/step - loss: 0.1616 - acc: 0.9563 - val_loss: 1.5775 - val_acc: 0.6691\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 568s 16s/step - loss: 0.1154 - acc: 0.9736 - val_loss: 1.7691 - val_acc: 0.6250\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 551s 16s/step - loss: 0.0746 - acc: 0.9791 - val_loss: 1.9119 - val_acc: 0.6507\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 699s 20s/step - loss: 0.0176 - acc: 0.9982 - val_loss: 2.0508 - val_acc: 0.6838\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 478s 14s/step - loss: 0.0089 - acc: 1.0000 - val_loss: 1.9776 - val_acc: 0.6765\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 514s 15s/step - loss: 0.0161 - acc: 0.9945 - val_loss: 2.1602 - val_acc: 0.6471\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 549s 16s/step - loss: 0.0884 - acc: 0.9754 - val_loss: 2.5919 - val_acc: 0.6029\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 545s 16s/step - loss: 0.2056 - acc: 0.9372 - val_loss: 1.9009 - val_acc: 0.6140\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 548s 16s/step - loss: 0.1599 - acc: 0.9490 - val_loss: 1.6536 - val_acc: 0.6140\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 540s 15s/step - loss: 0.0610 - acc: 0.9827 - val_loss: 1.6941 - val_acc: 0.6654\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 547s 16s/step - loss: 0.0161 - acc: 0.9982 - val_loss: 2.0764 - val_acc: 0.6507\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 554s 16s/step - loss: 0.0050 - acc: 1.0000 - val_loss: 2.1418 - val_acc: 0.6654\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 543s 16s/step - loss: 0.0014 - acc: 1.0000 - val_loss: 2.1602 - val_acc: 0.6765\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 548s 16s/step - loss: 7.2933e-04 - acc: 1.0000 - val_loss: 2.2187 - val_acc: 0.6654\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 543s 16s/step - loss: 5.5561e-04 - acc: 1.0000 - val_loss: 2.3916 - val_acc: 0.6691\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - 550s 16s/step - loss: 4.5500e-04 - acc: 1.0000 - val_loss: 2.3149 - val_acc: 0.6691\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - 560s 16s/step - loss: 3.8010e-04 - acc: 1.0000 - val_loss: 2.3234 - val_acc: 0.6691\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - 561s 16s/step - loss: 3.3837e-04 - acc: 1.0000 - val_loss: 2.3770 - val_acc: 0.6691\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - 555s 16s/step - loss: 2.9938e-04 - acc: 1.0000 - val_loss: 2.4454 - val_acc: 0.6691\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - 541s 15s/step - loss: 2.6683e-04 - acc: 1.0000 - val_loss: 2.3466 - val_acc: 0.6691\n",
      "Epoch 29/100\n",
      "35/35 [==============================] - 542s 15s/step - loss: 2.3319e-04 - acc: 1.0000 - val_loss: 2.3439 - val_acc: 0.6691\n",
      "Epoch 30/100\n",
      "35/35 [==============================] - 544s 16s/step - loss: 2.1392e-04 - acc: 1.0000 - val_loss: 2.4561 - val_acc: 0.6691\n",
      "Epoch 31/100\n",
      "35/35 [==============================] - 540s 15s/step - loss: 1.8909e-04 - acc: 1.0000 - val_loss: 2.4900 - val_acc: 0.6691\n",
      "Epoch 32/100\n",
      "35/35 [==============================] - 543s 16s/step - loss: 1.8349e-04 - acc: 1.0000 - val_loss: 2.4910 - val_acc: 0.6691\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - 552s 16s/step - loss: 1.6375e-04 - acc: 1.0000 - val_loss: 2.5228 - val_acc: 0.6691\n",
      "Epoch 34/100\n",
      "35/35 [==============================] - 538s 15s/step - loss: 1.4821e-04 - acc: 1.0000 - val_loss: 2.4668 - val_acc: 0.6691\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - 545s 16s/step - loss: 1.3559e-04 - acc: 1.0000 - val_loss: 2.4845 - val_acc: 0.6691\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - 542s 15s/step - loss: 1.2711e-04 - acc: 1.0000 - val_loss: 2.4938 - val_acc: 0.6691\n",
      "Epoch 37/100\n",
      "35/35 [==============================] - 538s 15s/step - loss: 1.2088e-04 - acc: 1.0000 - val_loss: 2.4097 - val_acc: 0.6691\n",
      "Epoch 38/100\n",
      "35/35 [==============================] - 540s 15s/step - loss: 1.1267e-04 - acc: 1.0000 - val_loss: 2.5739 - val_acc: 0.6691\n",
      "Epoch 39/100\n",
      "35/35 [==============================] - 542s 15s/step - loss: 1.0327e-04 - acc: 1.0000 - val_loss: 2.5096 - val_acc: 0.6691\n",
      "Epoch 40/100\n",
      "35/35 [==============================] - 549s 16s/step - loss: 9.7498e-05 - acc: 1.0000 - val_loss: 2.6428 - val_acc: 0.6691\n",
      "Epoch 41/100\n",
      "35/35 [==============================] - 541s 15s/step - loss: 9.5750e-05 - acc: 1.0000 - val_loss: 2.5539 - val_acc: 0.6691\n",
      "Epoch 42/100\n",
      "35/35 [==============================] - 540s 15s/step - loss: 8.6303e-05 - acc: 1.0000 - val_loss: 2.6545 - val_acc: 0.6728\n",
      "Epoch 43/100\n",
      "35/35 [==============================] - 537s 15s/step - loss: 8.1816e-05 - acc: 1.0000 - val_loss: 2.6553 - val_acc: 0.6691\n",
      "Epoch 44/100\n",
      "35/35 [==============================] - 579s 17s/step - loss: 7.8735e-05 - acc: 1.0000 - val_loss: 2.5784 - val_acc: 0.6728\n",
      "Epoch 45/100\n",
      "35/35 [==============================] - 546s 16s/step - loss: 7.2176e-05 - acc: 1.0000 - val_loss: 2.5439 - val_acc: 0.6691\n",
      "Epoch 46/100\n",
      "35/35 [==============================] - 551s 16s/step - loss: 6.9680e-05 - acc: 1.0000 - val_loss: 2.6286 - val_acc: 0.6691\n",
      "Epoch 47/100\n",
      "35/35 [==============================] - 546s 16s/step - loss: 6.5496e-05 - acc: 1.0000 - val_loss: 2.5660 - val_acc: 0.6691\n",
      "Epoch 48/100\n",
      "35/35 [==============================] - 570s 16s/step - loss: 6.2207e-05 - acc: 1.0000 - val_loss: 2.5690 - val_acc: 0.6728\n",
      "Epoch 49/100\n",
      "35/35 [==============================] - 579s 17s/step - loss: 5.9399e-05 - acc: 1.0000 - val_loss: 2.7902 - val_acc: 0.6728\n",
      "Epoch 50/100\n",
      "35/35 [==============================] - 1021s 29s/step - loss: 5.7168e-05 - acc: 1.0000 - val_loss: 2.6337 - val_acc: 0.6691\n",
      "Epoch 51/100\n",
      "35/35 [==============================] - 1466s 42s/step - loss: 5.4191e-05 - acc: 1.0000 - val_loss: 2.6739 - val_acc: 0.6728\n",
      "Epoch 52/100\n",
      "35/35 [==============================] - 935s 27s/step - loss: 5.1765e-05 - acc: 1.0000 - val_loss: 2.7296 - val_acc: 0.6728\n",
      "Epoch 53/100\n",
      "35/35 [==============================] - 671s 19s/step - loss: 4.9169e-05 - acc: 1.0000 - val_loss: 2.6591 - val_acc: 0.6728\n",
      "Epoch 54/100\n",
      "35/35 [==============================] - 663s 19s/step - loss: 4.9030e-05 - acc: 1.0000 - val_loss: 2.5934 - val_acc: 0.6691\n",
      "Epoch 55/100\n",
      "35/35 [==============================] - 754s 22s/step - loss: 4.5711e-05 - acc: 1.0000 - val_loss: 2.7504 - val_acc: 0.6691\n",
      "Epoch 56/100\n",
      "26/35 [=====================>........] - ETA: 7:29 - loss: 4.3437e-05 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator,\n",
    "                              epochs=100,\n",
    "                              verbose=1,\n",
    "                              validation_data=validation_generator)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "acc=history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "\n",
    "# Plot training and validation accuracy per epoch\n",
    "\n",
    "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "# Plot training and validation loss per epoch\n",
    "\n",
    "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
    "\n",
    "\n",
    "plt.title('Training and validation loss')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
